{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¹ Data Cleaning and Preprocessing\n",
    "\n",
    "This notebook handles:\n",
    "- Loading raw startup data from CSV/JSON files\n",
    "- Cleaning and validating data\n",
    "- Transforming data into MongoDB-compatible format\n",
    "- Preparing data for ingestion into MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('..')\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "\n",
    "Load startup data from various sources (CSV, JSON, API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load from CSV\n",
    "def load_csv_data(filepath):\n",
    "    \"\"\"Load startup data from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"âœ… Loaded {len(df)} records from {filepath}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "# Example: Load from JSON\n",
    "def load_json_data(filepath):\n",
    "    \"\"\"Load startup data from JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"âœ… Loaded {len(data)} records from {filepath}\")\n",
    "        return pd.DataFrame(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "# For demonstration, create sample data\n",
    "sample_data = {\n",
    "    'name': ['TechCorp', 'FinanceAI', 'HealthPlus', 'EduTech', 'GreenEnergy'],\n",
    "    'founded_year': [2018, 2019, 2017, 2020, 2016],\n",
    "    'country': ['USA', 'UK', 'Germany', 'India', 'USA'],\n",
    "    'city': ['New York', 'London', 'Berlin', 'Bangalore', 'San Francisco'],\n",
    "    'industry': ['SaaS,Enterprise', 'FinTech,AI', 'HealthTech', 'EdTech,AI', 'CleanTech,Energy'],\n",
    "    'total_funding_usd': [5000000, 12000000, 8000000, 3000000, 15000000],\n",
    "    'employee_count': [50, 120, 80, 30, 200],\n",
    "    'status': ['Operating', 'Operating', 'Operating', 'Operating', 'Acquired']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(\"\\nðŸ“Š Sample Data Loaded:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"ðŸ“‹ Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nðŸ” Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found!\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated(subset=['name']).sum()\n",
    "print(f\"\\nðŸ”„ Duplicate company names: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_startup_data(df):\n",
    "    \"\"\"Clean and transform startup data.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates(subset=['name'], keep='first')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean['city'] = df_clean['city'].fillna('Unknown')\n",
    "    df_clean['employee_count'] = df_clean['employee_count'].fillna(0).astype(int)\n",
    "    \n",
    "    # Convert industry string to list\n",
    "    df_clean['industry'] = df_clean['industry'].apply(\n",
    "        lambda x: [i.strip() for i in x.split(',')] if pd.notna(x) else []\n",
    "    )\n",
    "    \n",
    "    # Ensure founded_year is integer\n",
    "    df_clean['founded_year'] = df_clean['founded_year'].astype(int)\n",
    "    \n",
    "    # Ensure total_funding_usd is numeric\n",
    "    df_clean['total_funding_usd'] = pd.to_numeric(df_clean['total_funding_usd'], errors='coerce').fillna(0)\n",
    "    \n",
    "    print(f\"âœ… Cleaned {len(df_clean)} records\")\n",
    "    return df_clean\n",
    "\n",
    "df_clean = clean_startup_data(df)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform to MongoDB Format\n",
    "\n",
    "Convert DataFrame to list of dictionaries suitable for MongoDB insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_mongodb_format(df):\n",
    "    \"\"\"Transform DataFrame to MongoDB document format.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        doc = {\n",
    "            'name': row['name'],\n",
    "            'founded_year': int(row['founded_year']),\n",
    "            'country': row['country'],\n",
    "            'city': row['city'],\n",
    "            'industry': row['industry'],\n",
    "            'total_funding_usd': float(row['total_funding_usd']),\n",
    "            'employee_count': int(row['employee_count']),\n",
    "            'status': row['status']\n",
    "        }\n",
    "        \n",
    "        # Add optional fields if available\n",
    "        if 'funding_rounds' in row and pd.notna(row['funding_rounds']):\n",
    "            doc['funding_rounds'] = row['funding_rounds']\n",
    "        \n",
    "        if 'investors' in row and pd.notna(row['investors']):\n",
    "            doc['investors'] = row['investors']\n",
    "        \n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(f\"âœ… Transformed {len(documents)} documents for MongoDB\")\n",
    "    return documents\n",
    "\n",
    "mongodb_documents = transform_to_mongodb_format(df_clean)\n",
    "\n",
    "# Display sample document\n",
    "print(\"\\nðŸ“„ Sample MongoDB Document:\")\n",
    "print(json.dumps(mongodb_documents[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON for MongoDB ingestion\n",
    "output_path = os.path.join(config.PROCESSED_DATA_DIR, 'cleaned_startups.json')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(mongodb_documents, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Saved {len(mongodb_documents)} documents to {output_path}\")\n",
    "\n",
    "# Also save as CSV for reference\n",
    "csv_output_path = os.path.join(config.PROCESSED_DATA_DIR, 'cleaned_startups.csv')\n",
    "df_clean.to_csv(csv_output_path, index=False)\n",
    "print(f\"âœ… Saved CSV to {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Data Quality Summary:\")\n",
    "print(f\"\\nâœ… Total records processed: {len(df_clean)}\")\n",
    "print(f\"âœ… Unique companies: {df_clean['name'].nunique()}\")\n",
    "print(f\"âœ… Countries represented: {df_clean['country'].nunique()}\")\n",
    "print(f\"âœ… Industries covered: {sum(len(industries) for industries in df_clean['industry'])}\")\n",
    "print(f\"âœ… Date range: {df_clean['founded_year'].min()} - {df_clean['founded_year'].max()}\")\n",
    "print(f\"\\nðŸ’° Total funding across all startups: ${df_clean['total_funding_usd'].sum():,.0f}\")\n",
    "print(f\"ðŸ‘¥ Total employees: {df_clean['employee_count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. âœ… Data has been cleaned and transformed\n",
    "2. âœ… Documents are ready for MongoDB ingestion\n",
    "3. ðŸ“¥ Use `mongodb_setup.py` to insert data into MongoDB Atlas\n",
    "4. ðŸ“Š Proceed to `analysis.ipynb` for exploratory data analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
